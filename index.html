<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SUPE leverages skills from unlabeled prior data for efficient online exploration in reinforcement learning, demonstrating significant improvements across 42 long-horizon sparse-reward tasks.">
  <meta name="keywords" content="SUPE, Reinforcement Learning, Exploration, Skills, Prior Data, Hierarchical RL, Offline RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SUPE: Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://rail.eecs.berkeley.edu/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wilcoxsonm21.github.io">Max Wilcoxson*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://colinqiyangli.github.io">Qiyang Li*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://kvfrans.com">Kevin Frans</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.18076"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.18076"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rail-berkeley/supe"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. 
          </p>
          <p>
            In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration.
          </p>
          <p>
            Our method <span class="dnerf"> SUPE</span> (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<div class="columns is-centered">
  <div class="column">
    <figure class="image">
      <img src="static/images/supe_method_graphic.png" alt="SUPE Method Overview" style="width: 100%; height: auto;">
    </figure>
  </div>
</div>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Challenges -->
      <div class="column">
        <h2 class="title is-3">Challenges</h2>
        <div class="content">
          <h4 class="title is-5">1) Unstructured Exploration</h4>
            <figure class="image is-fullwidth" style="margin: 0; margin-left: -0.4rem; margin-right: -1.5rem;">
              <img src="static/images/temp_uncorrelated_website.png" alt="Temporally Incoherent Exploration" style="width: 100%; height: auto;">
            </figure>
          
          <p>
            With no initial reward information, the RL agent does not know which direction it should take and naively sampling actions from the agent results in temporally incoherent exploration. In many long-horizon tasks, this can lead to difficulty discovering high-reward states, resulting in high sample complexity. <strong>How can we structure exploration with temporally coherent actions?</strong>
          </p>
          
          <h4 class="title is-5">2) No Access to Task-Specific Reward Labels</h4>
          <figure class="image is-fullwidth" style="margin: 0; margin-left: -0.4rem; margin-right: -1.5rem;">
            <img src="static/images/pessimistic_reward_website.png" alt="Temporally Incoherent Exploration" style="width: 100%; height: auto;">
          </figure>
          <p>
            Labelled prior data can accelerate online learning by identifying high reward behaviors. However, task-specific prior data can be costly to collect, and is often not available for the task at hand. Naïvely learning a reward model online to obtain estimated rewards for the offline data works poorly, since estimates for offline transitions not in the offline data distribution are pessimistic, discouraging exploration. <strong>How should we estimate dataset rewards?</strong>
          </p>
        </div>
      </div>

      <!-- Method -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Method</h2>
          <p>
            To address these challenges, our method uses unlabelled prior data in two distinct ways:
          </p>
          
          <h4 class="title is-5">1) Skill Extraction</h4>
          <p>
            To structure online exploration, we extract skills from the data to capture low-level, task-agnostic behaviors. We use VAE skill pre-training, where trajectory segments from the data are passed into a variational auto encoder which encodes the trajectory into a latent vector z. Then, the decoder serves as the skill policy, where it reconstructs each low-level action in the original trajectory segment based on the latent vector z and the current state associated with the low-level action. For example, in a maze environment where the low-level actions are the control joint forces, the skills may be motion primitives within the maze. This allows exploration to stay within the space of useful behaviors present in the prior data — ie. moving in different directions, rather than applying random forces to the agent.
          </p>
          
          <h4 class="title is-5">2) Optimistic Relabelling</h4>
          <p>
            We then label offline transitions with an optimistic reward estimate, and use them as additional off-policy data during online learning to encourage exploration. More concretely, we need to convert unlabelled low-level transitions into labeled meta transitions. This requires two steps:
          </p>
          <ol style="margin-left: 1.5rem;">
            <li>First, we estimate the latent action of the offline trajectory segment using the encoder from the pre-trained VAE</li>
            <li>Second, we estimate the reward using a reward model learned online, plus an uncertainty bonus estimated using RND.</li>
          </ol>
          <p>
            This causes the entire offline data distribution to have an optimistic reward estimate, encouraging the agent to visit these areas.
          </p>
          <p>
            Putting these two components together with online RL training, our method, SUPE, trains a high-level policy on a combination of the online replay buffer data and optimistically labeled offline data to output a high-level action every H steps. The pre-trained policy enables temporally coherent exploration and the optimistic data labeling improves the sample efficiency of policy learning.
          </p>
        </div>
      </div>
    </div>

    <!-- Key Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Key Results</h2>
        <div class="content">
          <p>
            SUPE demonstrates significant improvements across multiple challenging domains:
          </p>
          <div class="columns">
            <div class="column">
              <div class="notification is-primary is-light">
                <p><strong>42 Tasks:</strong> Consistent outperformance across long-horizon, sparse-reward tasks</p>
              </div>
            </div>
            <div class="column">
              <div class="notification is-info is-light">
                <p><strong>AntMaze:</strong> Superior exploration efficiency in complex navigation environments</p>
              </div>
            </div>
          </div>
          <div class="columns">
            <div class="column">
              <div class="notification is-success is-light">
                <p><strong>OGBench:</strong> Strong performance on humanoid maze, manipulation, and soccer tasks</p>
              </div>
            </div>
            <div class="column">
              <div class="notification is-warning is-light">
                <p><strong>Robustness:</strong> Effective across different offline data qualities and corruptions</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Experimental Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experimental Results</h2>
        
        <h3 class="title is-4">Comprehensive Evaluation</h3>
        <div class="content has-text-justified">
          <p>
            SUPE was evaluated across 42 challenging tasks spanning multiple domains:
          </p>
          <ul>
            <li><strong>AntMaze (D4RL):</strong> 18 navigation tasks with different maze layouts and goals</li>
            <li><strong>OGBench:</strong> 24 tasks including humanoid navigation, object manipulation, and soccer</li>
          </ul>
        </div>

        <h3 class="title is-4">Key Findings</h3>
        <div class="columns">
          <div class="column">
            <div class="box">
              <h4 class="title is-5">Exploration Efficiency</h4>
              <p>SUPE finds sparse rewards significantly faster than baselines, often discovering goals that other methods fail to reach entirely.</p>
            </div>
          </div>
          <div class="column">
            <div class="box">
              <h4 class="title is-5">Sample Efficiency</h4>
              <p>By leveraging both skill pretraining and pseudo-labeled offline data, SUPE achieves better sample efficiency than methods using either technique alone.</p>
            </div>
          </div>
          <div class="column">
            <div class="box">
              <h4 class="title is-5">Robustness</h4>
              <p>SUPE maintains strong performance across different offline data qualities, from expert demonstrations to random exploratory data.</p>
            </div>
          </div>
        </div>

        <h3 class="title is-4">Ablation Studies</h3>
        <div class="content has-text-justified">
          <p>
            The paper includes comprehensive ablation studies demonstrating:
          </p>
          <ul>
            <li>The importance of using offline data twice (both offline and online phases)</li>
            <li>Sensitivity analysis across different hyperparameters</li>
            <li>Robustness to data corruptions and insufficient coverage</li>
            <li>Performance across different offline dataset qualities</li>
          </ul>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wilcoxson2024supe,
  title={Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration},
  author={Wilcoxson, Max and Li, Qiyang and Frans, Kevin and Levine, Sergey},
  journal={arXiv preprint arXiv:2410.18076},
  year={2024}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2410.18076">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/rail-berkeley/supe" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/wilcoxsonm21/wilcoxsonm21.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
